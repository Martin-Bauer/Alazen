% !TEX root = Projektspezifikation.tex
\section{Integration}
\subsection{Integrationsprozess}
\subsubsection{Ablauf der Integration}
Die Software wird standardmäßig zwei Quellen integriert haben: die dbSNP und das 1000GenomeProject. Diese werden, über ein Skript gesteuert, heruntergeladen. Wenn dieser Part abgeschlossen ist, wird ein lokaler Parser (später mehr zu den Parsern und ihrer Arbeitsweise) alle relevanten Daten, d.h. nur menschliche Daten, und aus diesen die, für das Projekt, relevanten Daten, aus den heruntergeladenen Dateien extrahieren und sie in einem, für den globalen Parser akzeptablen Format bereitstellen. Dies ist wichtig, da die Quellen nicht nur menschliche Daten bereitstellen, und auch viele weitere Daten vorhanden sind, die für das Projekt von keiner Relevanz sind. Der globale Parser wird diese Dateien dann in Datensätze staffeln und diese dann in das Data Ware House einfügen um sie dann so der Middleware für die Abfragen bereitzustellen.\\
Der lokale Parser wird benötigt, da es kein allgemeingültiges Standardformat gibt, in dem die Dateien abgespeichert werden. Die Formate sind auch keine, von dem DWH anerkannten, Dateiformate, die integriert werden können. Hinzu kommt, dass Sehr viele weitere Daten, die nicht notwendig sind, in den Dateien vorhanden sind und somit nur die benötigten Daten ausgelesen werden müssen.\\
Wenn der User weitere Quellen integrieren möchte, benötigt er zwei Dinge dafür: Ein Downloadskript, welches von der gewünschten Quelle die Dateien herunterlädt, und ein lokaler Parser, der die heruntergeladenen Dateien danach für den globalen Parser bereitstellt.
\subsubsection{Quellenauswahl}
Die Quellen, die von uns integriert sein werden, werden die dbSNP und das 1000GenomeProject sein, da diese alle benötigten Daten frei zugänglich bereitstellen, ohne jegliche Gegenleistung zu verlangen. 1000GenomeProject stellt ausschließlich menschliche Daten mit Metadaten und Referenzgenom zur Verfügung, dbSNP stellt nur Mutationsdaten und das Referenzgenom bereit, ohne Metadaten, jedoch auch tierische Daten, wobei für uns nur die menschlichen von Relevanz sind. HGMD hingegen können wir nicht integrieren, da für die Benutzung ihrer Daten eine Lizenz erworben werden muss, die den finanziellen Rahmen des gesamten Projektes sprengt. TCGA bezieht sich auf ein weiteres Referenzgenom, weshalb wir die Integration dieser Quelle auslassen werden. Wir werden uns vorerst auf die frei zugänglichen Quellen konzentrieren, um eine Basis an Daten bereitstellen zu können. Weitere Quellen werden im Nachhinein integrierbar sein, weshalb die Option offen steht, TCGA im Nachhinein zu integrieren.
\subsubsection{Attributauswahl und -mapping}
Beide Basis-Datenbanken werden von dem gleichen Anbieter gehostet, was beiden Quellen ein relativ uniformes Format gibt. So stellen beide Quellen die Daten in BAM-Dateien und vcf-Dateien bereit, wobei wir uns auf die vcf-Dateien berufen, da diese die Daten beinhalten, die wichtig sind für die Abfragen, und auch gleichzeitig einfacher zu entschlüsseln sind, als die BAM-Dateien.\\
Wir entschieden uns, dass das Referenzgenom in einer Extradatei abgespeichert wird, um übermäßigen Traffic im DWH zu vermeiden. Des weiteren werden im DWH folgende Daten der Middleware bereitgestellt: Eine einzigartige ID für jeden Eintrag, die Mutationssequenz selber, die Koordinaten, wo im Referenzgenom die Mutation auftritt, der Name der Quelle, um eine schnelle Einordnung nach Quellen zu gewährleisten, der Name des Referenzgenoms, eine Angabe, in welchem Chromosom die Mutation auftritt, und einen Verweis auf den entsprechenden Metadatensatz der Mutation. Metadaten bedeuten in unserem Falle, zusätzliche Informationen zur Person, bei der diese Mutation gefunden wurde.\\
Der Metadatensatz besteht aus einer eindeutigen ID, der Quelle, woher die Daten stammen, dem Geschlecht des Testsubjekts, dem Herkunftsland und der Downloadzeit. Wir entschieden uns für diese Metadaten, da sie in dem 1000GenomeProject bereitgestellt wird. Die dbSNP stellt keine Metadaten bereit.\\
\\
Im folgenden ein Ausschnitt aus einem Datensatz einer Mutation beider Quelldateien: \textit{17	52	rs556541063	C	A}. Danach kommt ein String mit Datenmüll - für das Projekt unrelevante Zusatzdaten - und dann hat die 1000GenomeProject-Quelle dahinter eine Liste mit Angaben, bei welchen Personen diese Mutation festgestellt wurde (somit also die Metadatenverweise). Der Ausschnitt hat folgende Werte: Das Chromosom, in dem die Mutation auftritt, die Position der Mutation, eine Quelldatenbankinterne ID für die Mutation, die wir jedoch nicht benötigen, referenzierte Stelle im Genom, und abschließend zu was sie mutierte. Im Falle einer Deletion wird dort nur ein "-" eingetragen sein, bei einer Insertion die entsprechend längere Basenfolge, eine Translokation zeigt die entsprechende mutierte Base an.
\subsubsection{Mengengerüst}
Eine Schätzung des Mengengerüstes ist für uns nicht möglich, da die Dateien der Quellen keine Schätzung zulassen. Die Dateien reichen in der Größe von einigen KB bis zu mehreren GB, die sehr stark anwachsen, wenn man sie entpackt (mindestens 15-fache Vergrößerung). In diesen Dateien kann man jedoch nicht approximieren, wie groß der Anteil an wichtigen Daten für das DWH ist, und wie viel Datenmüll, d.h. Daten, die für unser Projekt irrelevant sind, enthalten ist, da das von Datei zu Datei variiert.\\
Der Speicherplatz, den die Daten im DWH einnehmen werden, wird sich nach dem Einfügen um das 1,5-fache erhöhen.\\
\subsubsection{Inputfile-Format vom lokalen Parser erzeugt}
Referenzgenomname:„Name des Referenzgenoms Bsp: GRCh38“\\
Quelle:„hier die Quelle angeben“\\
\$\$\\
SampleID:„SampleID aus der DB“\\
Genkoordinaten:„Angabe der Koordinaten“ \\
Mutationssequenz:„Sequenz“\\
\$\$\\
SampleID:„SampleID aus der DB“\\
Gender:„m oder f“\\
Population:„drei Buchstaben bsp: GBR“ \\
EOF\\
\\
Der lokale Parser wird für jeden Datensatz einen Eintrag in dieser Art in sein Inputfile schreiben, so dass er, nach erfolgreichem Parsen eine Textdatei mit entsprechend vielen Einträgen dieser Art hat. Die Dollarzeichen kennzeichnen eine Trennung zwischen Datengruppen, erster Teil sind allgemeine Daten, zweiter Teil Mutationsdaten, dritter Teil Metadaten. Es gibt zwei Mal einen SampleID Eintrag, um Fehler leichter zu finden und das umformen in ein DWH-akzeptables Inputformat zu erleichtern.\\
Eine EBNF liegt im Anhang bereit.\\
\subsubsection{Inputfile vom globalen Parser erzeugt}
Es werden zwei Textdateien erstellt werden, eine für die Metadaten-Datensätze und eine für die Mutations-Datensätze. Diese werden danach per COPY-Befehl vom DWH eingelesen. Für diesen Befehl benötigen beide Dateien einen bestimmten Aufbau: Jedes Attribut ist voneinander per Leerzeichen getrennt und jede Zeile beherbergt einen Datensatz. Folgende Einträge beispielhafte Einträge wären somit möglich:\\
Mutationsdatei:\\
\textit{1 - 10456 10457 dbSNP GRCh37 17 1}\\
\\
Metadatendatei:\\
\textit{1 dbSNP w GER 12/12/2015}\\
\\
Die Einträge werden Zeilenweise in die entsprechende Tabelle eingefügt, Jeder Zelleneintrag durch ein Leerzeichen getrennt.
\subsubsection{Sequenzdiagramm}
\includepicture[width=\textwidth]{integration/SQDiag.png}{Sequenzdiagramm des Integrationsprozesses}
Der Ablauf besteht aus zwei Schleifen: Zuerst wird der gesamte Prozess von hinten gestartet, um zu gewährleisten, dass vor Beginn des Integrationsprozesses alle Teilprozesse laufen um Probleme während des Prozesses zu vermeiden. Der globale Parser wird gestartet, startet von sich aus die $n$ lokalen Parser, welche dann die Daten ihrer $n$ Quellen herunterladen. Jede Quelle hat einen, eigens für sich geschriebenen, lokalen Parser. Nach dem erfolgreichen Herunterladen parsen die $n$ lokalen Parser ihre Daten in jeweils ein Inputfile. Somit haben wir $n$ Inputfiles. Wenn diese Schleife beendet ist, wird der globale Parser alle $n$ Inputfiles einlesen und aus diesen $n$ Dateien zwei Textdateien erstellen - eine Metadaten-Datei und eine Mutations-Datei. Diese zwei Dateien, aufgebaut wie in 2.1.6, werden dann vom DWH per COPY-Befehl eingelesen, die Metadaten zuerst, um Probleme mit der Relation zu verhindern, danach die Mutationen.
\newpage
\subsection{Datenbankentwurf}
\includepicture[width=0.5\textwidth]{integration/DB.png}{Entwurf der Datenbank}
Die, bereits bei dem Attributmapping genannten, Attribute werden in zwei Tabellen gestaffelt, um eine effizientere und weniger fehleranfällige Struktur zu haben. Man kann sich möglicherweise sogar Einträge sparen, da es vorkommen kann, dass mehrere Mutationen von der gleichen Person stammen. Außerdem können die Anfragen der Middleware somit auch besser durchgeführt werden.\\
Erklärungen der Attribute der "Mutation"-Tabelle:\\
\\
\begin{tabular}{|l|c|c|c|c|r|}
\hline
Name & Typ & Format & Wertebereich & Beispiel & Notwendigkeit\\
\hline
MutationID & integer & i & 0 - 2.147.483.647 & 1, 2, 3, ...& notwendig\\
\hline
Mutation & text & String & beliebig lang & 'ATTCGATTAGCAGT' & notwendig\\
\hline
Mutationsanfang & bigint & i & 0 - 9.223.372.036.854.775.807 & 60000 & notwendig\\
\hline
Mutationsende & bigint & i & 0 - 9.223.372.036.854.775.807 & 80000 & notwendig\\
\hline
Quelle & text & String & beliebig lang & 'dbSNP' & notwendig\\
\hline
Referenzname & text & String & beliebig lang & 'GRCh38' & notwendig\\
\hline
Chromosom & char(2) & cc & X, Y oder 0 - 99 & 'X', 'Y', '64' & nicht notwendig\\
\hline
MetadatenID & int & i & 0 - 2.147.483.647 & 1, 2, 3, ... & notwendig\\
\hline
\end{tabular}\\
\\
Erklärungen der Attribute der "Metadaten"-Tabelle:\\
\begin{tabular}{|l|c|c|c|c|r|}
\hline
Name & Typ & Format & Wertebereich & Beispiel & Notwendigkeit\\
\hline
MetadatenID & int & i & 0 - 2.147.483.647 & 1, 2, 3, ... & notwendig\\
\hline
Quelle & text & String & beliebig lang & 'dbSNP', '1000GenomeProject' & notwendig\\
\hline
Geschlecht & char & c & m oder w & 'm', 'w' & nicht notwendig \\
\hline
Herkunft & text & Länderkürzel & Länderkürzel (meist dreistellig) & 'GBR' & nicht notwendig\\
\hline
Downloadzeit & date & dd/mm/yyyy & Bis zum Jahr 5874897 n.Chr. & '24/12/2015' & nicht notwendig, wird aber per Default auf heutigen Tag eingetragen\\
\hline
\end{tabular}\\
\\
Wir entschieden uns für diesen Aufbau, da die Abfragen des Users sich in zwei Bereichen unterteilt: Die Mutation selber und die dazugehörigen Metadaten. Eine Separierung dieser beiden Teilgruppen gibt der Middleware mehr Möglichkeiten schneller und effizienter das gewünschte Ergebnis zu finden. Dabei muss aber beachtet werden, dass es noch eine Relation zwischen den beiden Tabellen geben muss, nämlich zwischen beiden MetadatenID-Einträgen, damit jede Mutation genau einen Metadaten-Datensatz zugewiesen bekommt. Dies war der Grund, weshalb wir uns für das relationale Datenbankschema entschieden. Dieses Datenbankschema gibt uns die Möglichkeit, eine Relation zwischen den Mutationen und den Metadaten zu erstellen.\\
Nach dieser Entscheidung benötigten wir ein Datenbankmanagementsystem, was den Ansprüchen des Projektes gerecht werden musste, das heißt, dass es effizient mit großen Datenmengen umgehen können muss. Unsere Entscheidung fiel auf PostgreSQL. Dieses DBMS kann problemlos viele Daten und vor allem große Datensätze speichern und verwalten, was ausschlaggebend für das gesamte Projekt ist.
\subsection{Entwurf des Parsers}
Alle Parser werden in Java geschrieben. Parser für weitere Quellen müssen auch in Java geschrieben sein. Jede Quelle wird einen eigenen Parser haben, der die nötigen Daten aus den heruntergeladenen Quelldateien herausliest und in ein entsprechendes Inputfile für den globalen Parser schreiben wird, den lokalen Parser. Es wird so viele lokale Parser geben, wie es Quellen gibt. Der lokale Parser wird aus jeder Datei, die ihm von seiner Quelle bereitgestellt wird, so viele Datensätze wie möglich parsen und in einem Inputfile für den globalen Parser abspeichern.\\
Der globale Parser wird danach sämtliche, von den lokalen Parsern erstellte, Inputfiles parsen und in zwei Textdateien abspeichern, eine mit den Mutations-Datensätzen, eine mit den Metadaten-Datensätzen. Das DWH wird diese zwei Dateien dann per COPY-Befehl einlesen. 
\subsection{Klassendigramm}
\includepicture[width=\textwidth]{integration/Klassendiagramm.png}{Klassendiagramm der Integration}
Das Klassendiagramm verdeutlicht den linearen Ablauf des Integrationsprozesses. Zuerst werden die Quellen per Skript heruntergeladen und dann an die lokalen Parser übergeben. Diese parsen die Dateien dann in Inputfiles und übergeben diese an den globalen Parser übergeben. Dieser parset diese Dateien in Datenfiles, die danach mit dem COPY-Befehl des DWH in selbiges integriert werden.
\subsection{Schnittstellenspezifikation}
\subsubsection{Schnittstelle: Integration - Middleware}
Die Schnittstelle zwischen der Integration und der Middleware besteht darin, dass sich die Middleware per SQL Anfragen die, vom Nutzer gewünschten Daten, aus dem DWH lädt. Erlaubt sind alle SQL-Anfragen, außer Tabellen-/DB-verändernde, da die den gesamten Aufbau und und somit auch die Funktionalität des Programms zerstören. Außerdem dient die Middleware als Bindeglied zwischen dem User und der Integration, da die Middleware die Einschränkungen der Anfrage des Users an die Middleware weitergibt und sich etwaige Auswahlmöglichkeiten an den User weitergibt.
\subsubsection{Schnittstelle: Integration - Benutzer}
Der User kann seine Anfragen nach speziellen Gesichtspunkten einschränken, welche er von der Integration gestellt kriegen muss. Diese Punkte wird die Middleware ihm übergeben.\\
Außerdem kann ein User weitere Quelldatenbanken integrieren, indem er ein Downloadskript und einen lokalen Parser in der Ordnerstruktur bereitstellt und sie in die Liste der Quellen einträgt.\\
\subsection{Tests}
\subsubsection{Unit-Tests}
Es gibt viele verschiedene Dinge, die getestet werden müssen. Bezüglich des lokalen Parsers, muss getestet werden, wie er  mit einem fehlerhaften Input verfährt, und ob er korrekten Input auch auf korrekte Art und Weise parsed. Sollte er fehlerhaften Input kriegen, soll er den Fehler in einer Extra-Fehlerdatei abspeichern.\\
Das gleiche muss auch für den globalen Parser getestet werden. Dieser soll sich wie der lokale Parser verhalten, sollte er einen fehlerhaften Input bekommen, muss er diesen Datensatz in einer separaten Fehlerdatei abspeichern, damit der Nutzer sich später diese fehlerhaften Datensätze selber überprüfen kann.\\
Nach diesem doppelten Fehlertest könnte angenommen werden, dass der Input für das DWH nun korrekt ist, sollte dies nicht der Fall sein, so besitzt das DBMS eine Funktion, die bei einem fehlerhaften Datensatz, das gesamte restliche Einlesen stoppt, jedoch alle bisher eingelesenen Datensätze gespeichert lässt.\\
Beide Parser werden auch dafür benutzt, um etwaige doppelte Einträge in den Inputfiles herauszufiltern, sodass keine in dem DWH abgespeichert werden.\\
Im Anhang finden sich einige Testfiles um solche Fälle zu überprüfen.
